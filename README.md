# ğŸ§  LLM Adversarial Testing Framework

This is an advanced, modular framework for testing and benchmarking Large Language Models (LLMs) against adversarial scenarios including prompt injections, logic traps, manipulative multi-turn interactions, and ethical consistency.

## ğŸ”§ Features

- âœ… Support for **OpenAI GPT-4, GPT-4o, GPT-3.5**, and **Gemini Pro** models
- ğŸ§ª Multi-turn adversarial **chained prompt testing**
- ğŸ“Š Model performance logging with intent, difficulty, and tags
- âš™ï¸ Smart evaluation using LLM-based scoring
- ğŸ–¥ï¸ Interactive Streamlit GUI for filtering and testing scenarios
- ğŸ—ƒï¸ Built-in JSON + CSV logging and result export

## ğŸ“ Project Structure

```
llm-adversarial-framework/
â”‚
â”œâ”€â”€ chain_tool_gui_openai_v2.py        # Final merged GUI
â”œâ”€â”€ .env                               # Store your API keys here
â”‚
â”œâ”€â”€ framework/
â”‚   â”œâ”€â”€ runner.py                      # Main test runner
â”‚   â”œâ”€â”€ runner_chained.py             # Multi-turn chaining
â”‚   â”œâ”€â”€ evaluator.py                  # Evaluation logic
â”‚   â”œâ”€â”€ evaluation_utils_openai_v1.py # OpenAI scoring helpers
â”‚   â”œâ”€â”€ utils.py                      # Prompt save/load, Gemini, logging
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chained_prompts.json          # Saved user prompts
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ model_responses.csv           # Logs test results
â”‚
â””â”€â”€ README.md
    HOW_TO_USE.md
```

## ğŸš€ Quick Start

```bash
conda create -n llm-adversarial-framework python=3.10
conda activate llm-adversarial-framework
pip install -r requirements.txt
streamlit run chain_tool_gui_openai_v2.py
```

## ğŸ” Environment Setup

Create a `.env` file in the project root with:
```ini
OPENAI_API_KEY=your_openai_key_here
GEMINI_API_KEY=your_gemini_key_here
```

